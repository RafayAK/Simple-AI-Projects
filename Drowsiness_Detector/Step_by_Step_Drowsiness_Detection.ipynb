{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drowsiness Detection\n",
    "\n",
    "Drowsiness detection is straight forward. \n",
    "\n",
    "1. Find face in the frame\n",
    "2. Apply landmark detection on it\n",
    "3. Extract the eyes\n",
    "4. Compute the ratio between eyelids to measure either open or close\n",
    "5. Sound an alarm if the eyes have been closed for a sufficentely long time\n",
    "\n",
    "\n",
    "## Let's get to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "from imutils.video import VideoStream\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sound_alarm` is a helper function to play an alarm sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import playsound\n",
    "\n",
    "# play an alarm sound \n",
    "def sound_alarm(path):\n",
    "    playsound.playsound(path)  # block=False -> play asynchronously \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eye_aspect_ratio` is a helper method to calculate the aspect ratio of eyes\n",
    "\n",
    "To visualize conside the following image from Soukupová and Čech’s 2016 paper Real-Time Eye Blink Detection using Facial Landmarks:\n",
    "\n",
    "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2017/04/blink_detection_plot.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    # compute the euclidean distances between the two sets of \n",
    "    # vertical eye landmarks (x,y)-coordinates\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    \n",
    "    # compute the euclidean distance between the horizonatal\n",
    "    # eye landmark\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    \n",
    "    # compute the eye aspect ratio(ear) \n",
    "    ear = (A+B)/(2*C)\n",
    "    \n",
    "    # return the eye aspect ratio\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define a few important constatnts that may need to be ___tuned to achieve better results___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two constants, one for the EAR to indicate \n",
    "# blink and then a second constant for the number of \n",
    "# consecutive frames the eye must be below the threshold \n",
    "# for the alarm\n",
    "EYE_AR_THRESH = 0.25\n",
    "EYE_AR_CONSEC_FRAMES = 48\n",
    "\n",
    "# initialize the frame counter as well as the a bool used to\n",
    "# indicate if the alarm is going off\n",
    "COUNTER = 0\n",
    "ALARM_ON = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to model\n",
    "path_to_landmark_model = \"..\\\\DNN_MODELS\\\\shape_predictor_68_face_landmarks.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and speed we'll use dlib's HOG-based face detector, ___but the DNN model is much more robust and should be used instead___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading facial landmark predictor...\n"
     ]
    }
   ],
   "source": [
    "# initialize dlib's face detector(HOG based) and the facial\n",
    "# landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(path_to_landmark_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the landmarks to extract look at the following image:\n",
    "\n",
    "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup-768x619.jpg\">\n",
    "\n",
    "\n",
    "Therefore, to extract the left and right eyes we simply need to slice the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the indices of the left and right eye\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally get to the main loop of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this to display images like a video in notebook\n",
    "from IPython.display import display , Image, clear_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-a992fd1b603d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# detect faces in grayscale image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mrects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupsample_num_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# loop over the face detections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start the video stream thread\n",
    "print(\"[INFO] starting video stream thread...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(1.0) # let frames buffer\n",
    "\n",
    "# loop over the frames\n",
    "while True:\n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it an convert it to gray scale\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detect faces in grayscale image\n",
    "    rects = detector(gray, upsample_num_times=0)\n",
    "    \n",
    "    # loop over the face detections \n",
    "    for rect in rects:\n",
    "        # determine the facial landmarks for the face region, then \n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "        # extract the left and right eye coordinates, then use the\n",
    "        # coordinates to compute the eye aspect ratio for both eyes\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "        \n",
    "        # Soukupová and Čech recommend averaging both eye aspect \n",
    "        # ratios together to obtain a better estimation\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "        \n",
    "        # compute the convex hull for both left and right eye, then\n",
    "        # visualize each of the eyes with a green outline\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0,255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull],-1, (0,255, 0), 1)\n",
    "        \n",
    "        # now check if the person in the frame is dozing off\n",
    "        if ear < EYE_AR_THRESH: \n",
    "            # eyes closed\n",
    "            COUNTER+=1\n",
    "            \n",
    "            \n",
    "            # if the eyes were closed for a sufficent number of frames\n",
    "            # then sound the alarm\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                # if the alarm is not on, turn it on\n",
    "                if not ALARM_ON:\n",
    "                    ALARM_ON = True\n",
    "                   \n",
    "                    # play alarm, on background thread\n",
    "                    t = Thread(target=sound_alarm, args=(\"alarm.wav\",))\n",
    "                    t.deamon = True\n",
    "                    t.start()\n",
    "                    \n",
    "                    # draw an alarm on the frame\n",
    "                    cv2.putText(frame, \"DROWSINESS ALERT!\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),2)\n",
    "            \n",
    "            # otherwise, the EAR is not below the blink thresh i.e. eyes\n",
    "            # open , so reset the counter and alarm\n",
    "        else:\n",
    "            COUNTER = 0\n",
    "            ALARM_ON = False\n",
    "                \n",
    "        # draw an counter on the frame\n",
    "        cv2.putText(frame, \"{}\".format(COUNTER), (10, 300),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255),2)\n",
    "            \n",
    "                \n",
    "        # draw the computed eye aspect ratio on the frame to help\n",
    "        # with debugging and setting the correct eye aspect ratio\n",
    "        # thresholds and frame counters\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30), \n",
    "                     cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        \n",
    "           \n",
    "            \n",
    "    # for jupyter first we need to save the image then dsplay it \n",
    "    # for a video like effect\n",
    "    cv2.imwrite('pic.jpg', frame) # first we save it\n",
    "    display(Image(\"pic.jpg\"))\n",
    "\n",
    "    clear_output(wait=True)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a bit of cleanup\n",
    "COUNTER=0\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
